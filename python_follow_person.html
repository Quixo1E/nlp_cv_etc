<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Python Script Documentation</title>
    <style>
        body { font-family: Arial, sans-serif; }
        h1, h2 { color: #333; }
        code { background-color: #f4f4f4; padding: 2px 4px; }
        pre { background-color: #f8f8f8; border: 1px solid #ddd; padding: 10px; }
    </style>
</head>
<body>
    <h1>follow_person_airsim.py</h1>
    <p>This document outlines the structure and functionality of the Python script <code>follow_person_airsim.py</code>.</p>

    <h2>Overview</h2>
    <p>This is the main proof of concept script for our autonomous flight. The program initializes a drone takeoff, then triggers a object detection loop via the Zed SDK. </p>
    <p>Once an object is detected, it's distance from the camera, as well as its required yaw angle from center are both calculated. Then the drone passes those parameters to the drone movement methods in modules.py to commence movement. </p>
    <p>Depending on the path planned for the drone, you will need to provide movement context per each waypoint for the drone to navigate. There is a sample for such a feature commented out in lines 130</p>

    <h2>Imports</h2>
    <p>The script uses the following Python modules:</p>
    <ul>
        <li><code>pyzed.sl</code> - ZED SDK for depth sensing and spatial mapping.</li>
        <li><code>cv2</code> - OpenCV for image processing.</li>
        <li><code>numpy</code> - Numerical operations on arrays.</li>
        <li><code>math</code> - Mathematical functions.</li>
        <li><code>timeit</code>, <code>time</code> - Measure execution time and delays.</li>
        <li>Other custom modules such as <code>modules</code>.</li>
    </ul>

    <h2>Main Functions</h2>
    <h3>zed.grab()</h3>
    <p>executes the network detector on the video stream. Nested in a while loop for continuous deployment.</p>
    <h3>main_detection</h3>
    <p>This is the stored detection model passed to the camera. The defaults in the script is the YOLOv5 lite model with the PERSON_HEAD categorizer (see lines 87). This network can be changed to a custom network provided the path.</p>
    <h3>obj_array[]</h3>
    <p>Contains the set of identified objects from the network. obj_array[0].position returns the xyz coordinates of the first waypoint and then passes those to the AirSim client movement module methods.</p>
    <h3>client.RotateToYawAsSync() + client.moveByVelocityAsync()</h3>
    <p>see modules doc</p>



    <h2>Script Execution</h2>
    <p>T</p>


    <h2>Conclusion</h2>
    <p>This document provides an overview of the <code>follow_person_airsim.py</code> script, outlining its main components and functionality. For a detailed understanding, reviewing the script's source code alongside this documentation is recommended.</p>
</body>
</html>
